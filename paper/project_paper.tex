%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 1.3 (9/9/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside]{article}

\usepackage{lipsum} % Package to generate dummy text throughout this template
\usepackage{graphicx} % fFgures
\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage{multicol} % Used for the two-column layout of the document
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they need to be placed in specific locations with the [H] (e.g. \begin{table}[H])
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\Roman{subsection}} % Roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Exploratory Analysis of the UCI Forest Covertype and NSL-KDD Datasets Using Hadoop, Machine Learning, and Big Data Techniques} % Custom header text
\fancyfoot[RO,LE]{\thepage : This course is developed and taught by Dr. Shan Suthaharan at the Department of Computer Science, University of North Carolina at Greensboro (UNCG) in Fall 2014. The development and delivery of this course is funded by the Center for Science of Information, Purdue University through a sub-award approved by the National Science Foundation, and partially funded by UNCG} % Custom footer text

% LISTINGS
\usepackage{listings} % Required for inserting code snippets
\usepackage[usenames,dvipsnames]{color} % Required for specifying custom colors and referring to colors by name

\definecolor{DarkGreen}{rgb}{0.0,0.4,0.0} % Comment color
\definecolor{highlight}{RGB}{255,251,204} % Code highlight color

\lstdefinestyle{Style1}{ % Define a style for your code snippet, multiple definitions can be made if, for example, you wish to insert multiple code snippets using different programming languages into one document
language=Perl, % Detects keywords, comments, strings, functions, etc for the language specified
backgroundcolor=\color{highlight}, % Set the background color for the snippet - useful for highlighting
basicstyle=\footnotesize\ttfamily, % The default font size and style of the code
breakatwhitespace=false, % If true, only allows line breaks at white space
breaklines=true, % Automatic line breaking (prevents code from protruding outside the box)
captionpos=b, % Sets the caption position: b for bottom; t for top
commentstyle=\usefont{T1}{pcr}{m}{sl}\color{DarkGreen}, % Style of comments within the code - dark green courier font
deletekeywords={}, % If you want to delete any keywords from the current language separate them by commas
%escapeinside={\%}, % This allows you to escape to LaTeX using the character in the bracket
firstnumber=1, % Line numbers begin at line 1
frame=single, % Frame around the code box, value can be: none, leftline, topline, bottomline, lines, single, shadowbox
frameround=tttt, % Rounds the corners of the frame for the top left, top right, bottom left and bottom right positions
keywordstyle=\color{Blue}\bf, % Functions are bold and blue
morekeywords={}, % Add any functions no included by default here separated by commas
numbers=left, % Location of line numbers, can take the values of: none, left, right
numbersep=10pt, % Distance of line numbers from the code box
numberstyle=\tiny\color{Gray}, % Style used for line numbers
rulecolor=\color{black}, % Frame border color
showstringspaces=false, % Don't put marks in string spaces
showtabs=false, % Display tabs in the code as lines
stepnumber=5, % The step distance between line numbers, i.e. how often will lines be numbered
stringstyle=\color{Purple}, % Strings are purple
tabsize=2, % Number of spaces per tab in the code
}

% Create a command to cleanly insert a snippet with the style above anywhere in the document
\newcommand{\insertcode}[2]{\begin{itemize}\item[]\lstinputlisting[caption=#2,label=#1,style=Style1]{#1}\end{itemize}} % The first argument is the script location/filename and the second is a caption for the listing

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{\vspace{-15mm}\fontsize{24pt}{10pt}\selectfont\textbf{Exploratory Analysis of the UCI Forest Covertype and NSL-KDD Datasets Using Hadoop, Machine Learning, and Big Data Techniques}} % Article title

%\author{
%\large
%\textsc{Harry Rybacki}\\[2mm] % Your name
%\normalsize University of North Carolina at Greensboro \\ % Your institution
%\normalsize \href{mailto:hrybacki@gmail.com}{hrybacki@gmail.com} % Your email address
%\vspace{-5mm}
%}
% \date{}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Insert title

\thispagestyle{fancy} % All pages have headers and footers

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------



%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\begin{multicols}{2} % Two-column layout throughout the main article text

%------------------------------
%	STUDENT
%------------------------------

\section*{student}
\textsc{Harry Rybacki}\\[2mm]
CSC495/693 - Big Data Analytics and Machine Learning 
\newline \href{mailto:hrybacki@gmail.com}{hrybacki@gmail.com}
%------------------------------
%	INSTRUCTOR
%------------------------------

\section*{instructor}
\textsc{Dr. Shan Suthaharan}\\[2mm]
Department of Computer Science, UNC-Greensboro
\newline \href{mailto:s\_suthah@uncg.edu}{s\_suthah@uncg.edu}

\end{multicols}

%------------------------------
%	ABSTRACT
%------------------------------

\begin{abstract}

\noindent This paper focuses on exploratory big data analysis of two datasets: The UCI Forest Covertype dataset and the NSL-KDD dataset. After a brief review of the background information in the field, both datasets will be explored using rudimentary statistical analysis. Specifically, we will analyze each dataset for inaccuracy, incompleteness, and imbalance. Next, the computing environment necessary for big data analysis will be discussed as well as providing a detailed installation guide for Hadoop with simple examples of its usage. Finally, big data and machine learning analysis techniques will be used with Hadoop and Python in an attempt to classify observations. Specifically, the Stochastic Gradient Decent (SGD) algorithm is used in collusion with feature hashing to allow for similar scaling of both the size of the dataset and the computational complexity of the analysis. These results will be compared against the aforementioned datasets having been manually altered to be inaccurate, incomplete, or imbalanced to give us a solid benchmark to rate the effectiveness of our algorithm and computing environment choices. This paper hopes to find that Hadoop will prove to be more effective than less distributed computing environments as well as show the effectiveness of SGD and feature hashing by comparing the known good dataset analysis to the manually altered dataset analysis.

\end{abstract}

\begin{multicols}{2} % Two-Column layout throughout main article text

%------------------------------
%	INTRODUCTION
%------------------------------
\section{Introduction}

TBA

%------------------------------
%	BACKGROUND
%------------------------------
\section{Background}

Managing big data is difficult. As the number of features grows, the rate at which observations are stored, and the data varies more the storage and processing becomes increasingly more complex[2]. As a result, a combination of technologies designed to assist in the storage and processing as well as a careful choice of analysis algorithms is essential to getting any use from the data. 

In his paper, Suthaharan recommends the use of Hadoop Distributed File Systems (HDFS) and Cloud Technologies to assist in the storing of data as well has the communication infrastructure of the analysis network. 

As mentioned by Dalessandro, as datasets grow linearly the computational complexity of
standard statistical analysis algorithms grows exponentially[1]. However, using the Stochastic Gradient Decent (SGD) algorithm can alleviate much of the computational complexity gains. Despite being less optimal on smaller datasets, SGD takes advantage of sparsity within datasets and searches for min/max of individual data points making it scale linearly with the dataset.

Furthermore, the use of feature hashing lessens the aforementioned problems surrounding high feature dimensionality. Although feature hashing degrades the quality of the data it allows for working with incredibly large, millions or billions, amounts of features[1] in a way that scales linearly.

%------------------------------
%	DATASETS -- ASSIGNMENT 1
%------------------------------

\section{Datasets}

\subsection{NSL-KDD Dataset}
The NSL-KDD Dataset, obtainable here: \url{http://nsl.cs.unb.ca/NSL-KDD/}, was constructed to resolve inherent problems in a similar dataset that
was built in 1999[3]. The previous dataset was imbalanced and lead to bias toward more frequent
attacks. This issue is however no longer present with the current dataset. Furthermore it is complete and there are no chunks of the dataset missing. We will have to assume that when this dataset was constructed the tools used to capture this information were accurate and the dataset was not tampered with in a way that would lead to inaccuracy.

With 125,973 observations, each of which has forty features, we can conclude that his is a high dimensional dataset. Furthermore, if an observer were to be collecting similar data in a live environment (an active network) the velocity of number of observations would increase at a non-trivial rate.

INSERT HISTOGRAM OF PROTOCOL TYPES

\subsection{UCI Forest Covertype Dataset}
The UCI Forest Covertype Dataset[4], obtainable here: \url{https://archive.ics.uci.edu/ml/datasets/Covertype}, was constructed for predictive modeling of forested lands the neighbor forested lands under the control of the original dataset owners. This dataset contains 581,012 observations each of which has fifty-four features. Relevant statistical information for each feature was easy to calculate. For example, feature 1, elevation, has a 581,012 observations, a mean of 2,959 meters, and a standard deviation of 280 meters. 

INSERT HISTOGRAM OF ELEVATION 

We can state that this dataset is not imbalanced as there are an equal number of , inaccurate, or incomplete.

%------------------------------
%	COMPUTING ENVIRONMENT -- ASSIGNMENT 2
%------------------------------

\section{Computing Environment}
I opted to set up my Hadoop environment using the online cloud hosting provider, Digital Ocean. Compared to similar services provided by Linode and AWS (Amazon Web Services), Digital Ocean is relatively inexpensive. Furthermore, having contacted individuals that have set up Hadoop environments using Digital Oceans services I know that not only do they offer outstanding customer support should I need it but also that Digital Ocean provides mostly-complete guides describing how to set up Hadoop. For example, \url{https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-on-ubuntu-13-10} provides thorough documentation for installing a single-node Hadoop environment on Ubuntu 13.10. I will be using and expanding upon this guide as a base for my setup with one exception, I will be using the current version of Ubuntu, 14.04.

The use of Digital Ocean servers allows me to access my Hadoop Environment from virtually anywhere using SSH. For my environment, I will be using a server allocated with 2GB of RAM, a 40GB SSD drive, and two processor cores. I may opt to upgrade to a larger Digital Ocean droplet (sever) if these resources seem inadequate.

\end{multicols}

\subsection{Hadoop Configuration}
\begin{enumerate}
	\item Set Up Server on Digital Ocean
	\begin{enumerate}
		\item Visit \url{http://digitalocean.com}
		\item Sign in -- If you have not created an account, follow the "CREATE ACCOUNT" instructions on the landing page.
		\item Create a Droplet
		\begin{enumerate}
			\item Click the green "CREATE"  button on the top left corner of your menu
			\item Enter a name in the "Droplet Hostname" Field
			\item Select the \$ 20/mo droplet option
			\item Select the New York region
			\item Select Ubuntu 14.04x64 from the "Select Image" options
			\item Click "Create Droplet" at the bottom
		\end{enumerate}
		\item SSH into your droplet
			\begin{enumerate}
				\item Check your email for a message from Digital Ocean and note the droplet <IP\_ADDRESS> and <ROOT\_PASSWORD>
				\item Open up your desired way to SSH into a remote server
				\item Enter the following into your terminal
				\insertcode{"scripts/do_server_output"}{Server Output from Digital Ocean}
			\end{enumerate}
	\end{enumerate}
	\item Install and Test Hadoop
	\begin{enumerate}
		\item Install Java
		\insertcode{"scripts/do_install_java"}{Server Output from Installing Java}
		\item Create and Setup SSH Certificates
		\insertcode{"scripts/do_setup_ssh"}{Server Output from Setting Up SSH}
		\item Download and Install Hadoop
		\insertcode{"scripts/do_download_hadoop"}{Server Output from Downloading Hadoop}
		\item Edit and Setup Configuration Files:
			\begin{enumerate}
				\item Determine location of Java libraries
				\insertcode{"scripts/do_get_java_dir"}{Server Output from Editing Determining Java Library Directory}
				\item Edit \textasciitilde/.bashrc
				\insertcode{"scripts/do_edit_bashrc"}{Server Output from Editing .bashrc}
				\item Populate environmental variables
				\insertcode{"scripts/do_source_bashrc"}{Server Output from Sourcing New Env. Variables}
				\item Edit  /usr/local/hadoop/etc/hadoop/hadoop-env.sh
				\insertcode{"scripts/do_edit_hadoop_env"}{Server Output from Editing hadoop\_env.sh}
				\item Edit /usr/local/hadoop/etc/hadoop/core-site.xml
				\insertcode{"scripts/do_edit_core-site"}{Server Output from Editing core-site.xml}
				\item Edit /usr/local/hadoop/etc/hadoop/yarn-site.xml
				\insertcode{"scripts/do_edit_yarn-site"}{Server Output from Editing yarn-site.xml}
				\item Create and Edit /usr/local/hadoop/etc/hadoop/mapred-site.xml
				\insertcode{"scripts/do_edit_mapred-site"}{Server Output from Editing mapred-site.xml}
				\item Edit /usr/local/hadoop/etc/hadoop/hdfs-site.xml
				\insertcode{"scripts/do_edit_hdfs-site"}{Server Output from Editing hdfs-site.xml}
			\end{enumerate}
		\item Format the Hadoop Filesystem. With our configuration files set up we are ready to format the HDFS so that we can start our dfs. 
		\insertcode{"scripts/do_format_hdfs"}{Server Output from Formatting HDFS}
		\item Start Hadoop
		\insertcode{"scripts/do_start_hadoop"}{Server Output from Starting Hadoopl}
		\item Confirm all Hadoop services are running
		\insertcode{"scripts/do_confirm_install"}{Server Output from Confirming Hadoop Install}
	\end{enumerate}
\end{enumerate}

\subsection{Programming Examples}

\subsection{MapReduce Examples}

\subsection{Standard Example}

\subsubsection{My Example}

\begin{multicols}{2} % Two-column layout throughout the main article text

%------------------------------
%	MACHINE LEARNING -- ASSIGNMENT 3
%------------------------------

\section{Machine Learning}

TBA - Assignment 3

%------------------------------
%	CONCLUSION
%------------------------------

\section{Conclusion}

TBA

%------------------------------
%	ACKNOWLEDGEMENTS
%------------------------------

\section{Acknowledgement}

\begin{enumerate}
  \item Dr. Shan Suthaharan, Associate Professor, University of North Carolina at Greensboro
\end{enumerate}

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template

\bibitem[1]{Dalessandro:2013dg}
Dalessandro (2013). Bring The Noise: Embracing Randomness is the Key to Scaling Up Machine Learning Algorithms

\bibitem[2]{Suthaharan:2013dg}
Suthaharan (2013). Big Data Classification: Problems and Challenges in Network Intrusion Prediction with Machine Learning

\bibitem[3] AADD: NSL-KDD Dataset

\bibitem[4] AADD: UCI Tree Cover Dataset
 
\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{multicols}

\end{document}
